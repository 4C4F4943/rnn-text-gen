{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#The imports below are to allow models to train for a restricted amount of time.\n",
    "#This is useful for training multiple models over night, as they would not need\n",
    "#to be manually stopped.\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "To prepare our data for use by our neural net, we first needed to split it into groups of data that follow specific rules. To streamline the process, we used the `Dataset` class to store and manage our input data. This class was responsible for splitting the data into strings of the correct length and for turning them into one hot encoded arrays that the neural net could better understand. We stored this pre-prepared data in a `Batch` object, which has `inputs` and `targets` attributes for our model to use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, seqs):\n",
    "        \"\"\"Create a batch using the sequence\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "            seqs: int[][]\n",
    "                The one-hot encoded sequences.\n",
    "        \"\"\"\n",
    "        self.inputs = [seq[:-1] for seq in seqs]\n",
    "        self.targets = [seq[1:] for seq in seqs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(\n",
    "            self,\n",
    "            filenames,\n",
    "            seq_length,\n",
    "            shuffle=True,\n",
    "            buffer_size=10000,\n",
    "    ):\n",
    "        \"\"\"Creates a dataset\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "            filenames: string\n",
    "                Path to one or more plain text files.\n",
    "                The file contents are concatenated in the given order.\n",
    "\n",
    "            seq_length: int\n",
    "                The length of the text sequence.\n",
    "\n",
    "            shuffle: boolean\n",
    "                Whether to shuffle the sequences for the batches.\n",
    "\n",
    "            buffer_size: int\n",
    "                 The number of elements from this dataset from which the new\n",
    "                 dataset will sample.\n",
    "        \"\"\"\n",
    "        text = ''\n",
    "        vocab = set()\n",
    "        for filename in filenames:\n",
    "            content = open(filename).read()\n",
    "            text += content\n",
    "            vocab = vocab.union(set(content))\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.char_to_ix = {c: i for i, c in enumerate(vocab)}\n",
    "        self.ix_to_char = list(vocab)\n",
    "        self.text = text\n",
    "        self.data = np.array([self.char_to_ix[c] for c in text])\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def batch(\n",
    "            self,\n",
    "            batch_size,\n",
    "            drop_remainder=True\n",
    "    ):\n",
    "        \"\"\"Batch the instances\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "            batch_size: int\n",
    "                The number of instances in a single batch.\n",
    "\n",
    "            drop_remainder: boolean\n",
    "                Whether the last batch should be dropped in the case its has\n",
    "                fewer than batch_size elements.\n",
    "        \"\"\"\n",
    "        n_seq = len(self.data) // self.seq_length\n",
    "        n_batch = n_seq // batch_size\n",
    "        seq_ids = np.arange(n_seq)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(seq_ids)\n",
    "        i = 0\n",
    "        for _ in range(n_batch):\n",
    "            seqs = [None] * batch_size\n",
    "            for j in range(batch_size):\n",
    "                k = seq_ids[i] * self.seq_length\n",
    "                seqs[j] = self._create_seq(k, k + self.seq_length + 1)\n",
    "                i += 1\n",
    "            yield Batch(seqs)\n",
    "        if not drop_remainder:\n",
    "            seqs = []\n",
    "            for j in range(n_seq % batch_size):\n",
    "                k = seq_ids[i] * self.seq_length\n",
    "                seqs[j] = self._create_seq(k, k + self.seq_length + 1)\n",
    "                i += 1\n",
    "            yield Batch(seqs)\n",
    "\n",
    "    def _create_seq(self, i, j):\n",
    "        return list(map(self._to_label, self.data[i:j]))\n",
    "\n",
    "    def _to_label(self, index):\n",
    "        label = np.zeros(self.vocab_size)\n",
    "        label[index] = 1.0\n",
    "        return label\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"One-hot encode the text\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "            text: string\n",
    "                The text to encode.\n",
    "\n",
    "        Returns\n",
    "        ======================================================================\n",
    "            seq: int[]\n",
    "                The one-hot encoded sequence.\n",
    "        \"\"\"\n",
    "        return [self._to_label(self.char_to_ix[c]) for c in text]\n",
    "\n",
    "    def decode(self, seq):\n",
    "        \"\"\"Decode the one-hot encoded sequence to text format\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "            text: string\n",
    "                The text to encode.\n",
    "\n",
    "        Returns\n",
    "        ======================================================================\n",
    "            seq: int[]\n",
    "                The one-hot encoded sequence.\n",
    "        \"\"\"\n",
    "        text = ''\n",
    "        for label in seq:\n",
    "            text += self.ix_to_char[np.argmax(label)]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text generator itself is stored in the `RNNTextGenerator` class. Among other things, storing the generator in the class allows the session helps prevent accidental data loss.\n",
    "\n",
    "The class also internalizes the methods needed to save and restore the model as a file. This allows for long term storage and quick retreaval of a file, as well as increasing the ease of using the weights for a model with a different sized input.\n",
    "\n",
    "The text generator does not take batches when training, however, and needs to be fed the inputs and targets seperately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTextGenerator:\n",
    "    \"\"\"A text generator using basic cell and dynamic rnn\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            seq_length,\n",
    "            vocab_size,\n",
    "            rnn_cell=tf.nn.rnn_cell.BasicRNNCell,\n",
    "            n_neurons=100,\n",
    "            optimizer=tf.train.AdamOptimizer,\n",
    "            learning_rate=0.001,\n",
    "            name='RNNTextGenerator',\n",
    "            logdir=None\n",
    "    ):\n",
    "        \"\"\"Initialize the text generator and contruct the tf graph\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        seq_length: int\n",
    "            The number of characters in a sequence.\n",
    "\n",
    "        vocab_size: int\n",
    "            The number of unique characters in the text.\n",
    "\n",
    "        rnn_cell: tf.nn.rnn_cell.*\n",
    "            A rnn cell from tensorflow.\n",
    "\n",
    "        n_neurons: int\n",
    "            The number of neurons in each RNN cell.\n",
    "\n",
    "        optimizer: tf.train.*Optimizer\n",
    "            An optimizer from tensorflow.\n",
    "\n",
    "        learning_rate:\n",
    "            A Tensor or a floating point value. The learning rate of the\n",
    "            optimizer.\n",
    "\n",
    "        name: string\n",
    "            The name of the net (for graph visualization in tensorboard).\n",
    "\n",
    "        logdir: string\n",
    "            The path to save the tensorflow summary.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.tf_graph = tf.Graph()\n",
    "        with self.tf_graph.as_default():\n",
    "            self.tf_sess = tf.Session()\n",
    "            # One-hot encoded input and targets\n",
    "            \"\"\"placeholder\n",
    "            Example\n",
    "            [\n",
    "                batch_0: [\n",
    "                    seq_0: [\n",
    "                        # encoded labels with 5 categories\n",
    "                        [0, 0, 0, 1, 0],  # i = 0\n",
    "                        [0, 0, 1, 0, 0],  # i = 1\n",
    "                    ],\n",
    "                    ...\n",
    "                ],\n",
    "                ...\n",
    "            ]\n",
    "            \"\"\"\n",
    "            self.tf_input = tf.placeholder(\n",
    "                tf.float32, shape=(None, seq_length, vocab_size)\n",
    "            )\n",
    "            self.tf_target = tf.placeholder(\n",
    "                tf.float32, shape=(None, seq_length, vocab_size)\n",
    "            )\n",
    "            with tf.variable_scope(name):\n",
    "                self.tf_rnn_cell = rnn_cell(n_neurons)\n",
    "                outputs, _ = tf.nn.dynamic_rnn(\n",
    "                    self.tf_rnn_cell,\n",
    "                    tf.cast(self.tf_input, tf.float32),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "                logits = tf.layers.dense(outputs, vocab_size)\n",
    "                self.tf_loss = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                        logits=logits,\n",
    "                        labels=self.tf_target,\n",
    "                    )\n",
    "                )\n",
    "                self.tf_train = optimizer(\n",
    "                    learning_rate=learning_rate\n",
    "                ).minimize(self.tf_loss)\n",
    "                self.tf_prob = tf.nn.softmax(logits)\n",
    "                self.tf_acc = tf.reduce_mean(tf.cast(\n",
    "                    tf.equal(\n",
    "                        tf.argmax(logits, 2),\n",
    "                        tf.argmax(self.tf_target, 2),\n",
    "                    ),\n",
    "                    tf.float32\n",
    "                ))\n",
    "                self.tf_saver = tf.train.Saver()\n",
    "                if logdir is not None:\n",
    "                    self.logger = tf.summary.FileWriter(logdir, self.tf_graph)\n",
    "            # Initialize the tf session\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "            self.tf_sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    def fit(self, dataset, epoch, batch_size):\n",
    "        \"\"\"Fit and train the classifier with a batch of inputs and targets\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        inputs: np.ndarray\n",
    "            A batch of input sequences.\n",
    "\n",
    "        targets: np.ndarray\n",
    "            A batch of target sequences.\n",
    "        \"\"\"\n",
    "        for _ in range(epoch):\n",
    "            for batch in dataset.batch(batch_size):\n",
    "                self.tf_sess.run(\n",
    "                    self.tf_train,\n",
    "                    feed_dict={\n",
    "                        self.tf_input: batch.inputs,\n",
    "                        self.tf_target: batch.targets,\n",
    "                    },\n",
    "                )\n",
    "        return self\n",
    "\n",
    "    def score(self, inputs, targets):\n",
    "        \"\"\"Get the score for the batch\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        inputs: np.ndarray\n",
    "            A batch of input sequences.\n",
    "\n",
    "        targets: np.ndarray\n",
    "            A batch of target sequences.\n",
    "\n",
    "        Returns\n",
    "        ======================================================================\n",
    "        accuracy: tf.float32\n",
    "            The accuracy on this batch.\n",
    "\n",
    "        loss: tf.float32\n",
    "            The loss on this batch.\n",
    "        \"\"\"\n",
    "        return self.tf_sess.run(\n",
    "            [self.tf_acc, self.tf_loss],\n",
    "            feed_dict={\n",
    "                self.tf_input: inputs,\n",
    "                self.tf_target: targets,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"Predict the probablities for the labels, for a batch of inputs\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        inputs: np.ndarray\n",
    "            A batch of input sequences.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ======================================================================\n",
    "        predictions: np.ndarray\n",
    "            A batch of sequences of probablities.\n",
    "        \"\"\"\n",
    "        return self.tf_sess.run(\n",
    "            self.tf_prob,\n",
    "            feed_dict={\n",
    "                self.tf_input: inputs,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def save(self, path='./model'):\n",
    "        \"\"\"Save the model\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        path: string\n",
    "            The path to store the model.\n",
    "        \"\"\"\n",
    "        self.tf_saver.save(\n",
    "            self.tf_sess,\n",
    "            path + '/' + self.name\n",
    "        )\n",
    "\n",
    "    def restore(self, path='./model'):\n",
    "        \"\"\"Restore the model\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        path: string\n",
    "            The path where the model is saved.\n",
    "        \"\"\"\n",
    "        self.tf_saver.restore(\n",
    "            self.tf_sess,\n",
    "            path + '/' + self.name\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(model, dataset, start_seq, length):\n",
    "        \"\"\"Generate the text using a saved model\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        model: RNNTextGenerator\n",
    "            The model to sample from.\n",
    "\n",
    "        dataset: Dataset\n",
    "            The dataset to encode and decode the labels.\n",
    "\n",
    "        start_seq: string\n",
    "            The character sequence to begin with.\n",
    "\n",
    "        length: int\n",
    "            The length of the generated text.\n",
    "\n",
    "        Returns\n",
    "        ======================================================================\n",
    "        text: string\n",
    "            The generated text.\n",
    "        \"\"\"\n",
    "        text = [None] * length\n",
    "        seq = dataset.encode(start_seq)\n",
    "        for i in range(length):\n",
    "            ix = np.random.choice(\n",
    "                range(dataset.vocab_size),\n",
    "                # pred[batch 0][last item in the sequence]\n",
    "                p=model.predict([seq])[0][-1]\n",
    "            )\n",
    "            x = np.zeros(dataset.vocab_size)\n",
    "            x[ix] = 1\n",
    "            del seq[0]\n",
    "            seq.append(x)\n",
    "            text[i] = x\n",
    "        return dataset.decode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the dataset generator\n",
    "To test our dataset generator, we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch_a_seq():\n",
    "        print(\"-----------Testing Dataset generator-----------\")\n",
    "        batch_size = 5\n",
    "        seq_length = 100\n",
    "        filename = '../data/alice.txt'\n",
    "        dataset = Dataset([filename], seq_length)\n",
    "        for batch in dataset.batch(batch_size):\n",
    "            assert(len(batch.inputs) == batch_size)\n",
    "            assert(len(batch.targets) == batch_size)\n",
    "            assert(len(batch.inputs[-1]) == seq_length)\n",
    "            assert(len(batch.targets[-1]) == seq_length)\n",
    "            assert(sum(batch.inputs[1][1]) == 1)\n",
    "test_batch_a_seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the text generator\n",
    "Before using any neural net, it's importamt to make sure that it is correctly processing the data it is given. To make sure this is the case, we fed our text generator some randomly generated data. While a fresh model will be needed to train on the text, this model is used to verify that the code is working correctly.\n",
    "In this case, we are checking to ensure the model is providing sufficient variance in its outputs. This shows that the model is opperating on its inputs and successfully completing its operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_label(vocab_size):\n",
    "    \"\"\"randomly assign a label\n",
    "    \"\"\"\n",
    "    label = np.random.randint(vocab_size)\n",
    "    seq = np.zeros(vocab_size)\n",
    "    seq[label] = 1.0\n",
    "    return seq\n",
    "\n",
    "\n",
    "def random_data(batch_size, seq_length, vocab_size):\n",
    "    \"\"\"generate random data\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for _ in range(batch_size):\n",
    "        labels = [random_label(vocab_size) for _ in range(seq_length + 1)]\n",
    "        inputs.append(labels[:-1])\n",
    "        targets.append(labels[1:])\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "def test_on_random_data():\n",
    "    print(\"---------------Testing text generator with randomly generated data-------------\")\n",
    "    seq_length = 10\n",
    "    vocab_size = 4\n",
    "    batch_size = 2\n",
    "    text_gen = RNNTextGenerator(\n",
    "        seq_length,\n",
    "        vocab_size,\n",
    "    )\n",
    "    print('first fit')\n",
    "    inputs, targets = random_data(batch_size, seq_length, vocab_size)\n",
    "    print('fit:', text_gen.fit(inputs, targets))\n",
    "    print('score:', text_gen.score(inputs, targets))\n",
    "    print('predictions:', text_gen.predict(inputs))\n",
    "    print('true targets:', targets)\n",
    "    text_gen.save()\n",
    "\n",
    "    seq_length = 5\n",
    "    text_gen = RNNTextGenerator(\n",
    "        seq_length,\n",
    "        vocab_size\n",
    "    )\n",
    "    text_gen.restore()\n",
    "\n",
    "def test_log():\n",
    "    print(\"-------------Testing logs---------------\")\n",
    "    seq_length = 10\n",
    "    vocab_size = 4\n",
    "    batch_size = 2\n",
    "    text_gen = RNNTextGenerator(\n",
    "        seq_length,\n",
    "        vocab_size,\n",
    "        logdir='./tf_logs'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on_random_data()\n",
    "test_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the model'spredictions as to what the target are have sufficient variance so as to provide seemingly random probabilities of each output. This suggests that the model, while in need of training, is making its predictions appropriatly.\n",
    "\n",
    "We also verified that tensorflow's logs were being stored in the correct location, as that is also managed by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "A short amount of training provides us with a model that is capable of forming multiple words and a few phrases, but not much more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"An end to end test using \n",
    " ALICE'S ADVENTURES IN WONDERLAND\n",
    "\"\"\"\n",
    "print(\"----------------Testing dataset Alice---------------\")\n",
    "seq_length = 25\n",
    "batch_size = 25\n",
    "learning_rate = 0.01\n",
    "epoch = 10\n",
    "dataset = Dataset(['../data/alice.txt'], seq_length)\n",
    "model = RNNTextGenerator(\n",
    "    seq_length,\n",
    "    dataset.vocab_size,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "for _ in range(epoch):\n",
    "    for batch in dataset.batch(batch_size):\n",
    "        model.fit(batch.inputs, batch.targets)\n",
    "model.save()\n",
    "\n",
    "start_seq = 'hello'\n",
    "model = RNNTextGenerator(\n",
    "    len(start_seq),\n",
    "    dataset.vocab_size,\n",
    ")\n",
    "\n",
    "model.restore()\n",
    "print('>>>>> {}'.format(start_seq), RNNTextGenerator.sample(\n",
    "    model,\n",
    "    dataset,\n",
    "    start_seq,\n",
    "    500\n",
    "))\n",
    "print('<<<<<<')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long term training:\n",
    "We then continued to train the same model on our dataset to see how well our model learned when it continued to be fed data from its dataset. \n",
    "\n",
    "Every so many epochs, we paused training to test our model by generating our models scores and generating a sample text. This information is stored for comparison purpouses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(\n",
    "    dataset,\n",
    "    learning_rate,\n",
    "    start_seed,\n",
    "    model_name = \"RNNTextGenerator\",\n",
    "    model_exists = True,\n",
    "    train_seq_length = 25,\n",
    "    epoch = 20,\n",
    "    time_limit = 10\n",
    "    ):\n",
    "    runs_for = timedelta(minutes=time_limit)\n",
    "    start_time = datetime.now()\n",
    "    while(runs_for > datetime.now() - start_time ):\n",
    "        #build model to train on\n",
    "        model = RNNTextGenerator(\n",
    "            train_seq_length,\n",
    "            dataset.vocab_size,\n",
    "            learning_rate=learning_rate,\n",
    "            name=model_name,\n",
    "        )\n",
    "        try:\n",
    "            #If a model exists, we will need to restore it before we begin training.\n",
    "            model.restore()\n",
    "        except:\n",
    "            #If no model already exists, we can afford to ignore this error.\n",
    "            pass\n",
    "        #train\n",
    "        for _ in range(epoch):\n",
    "            for batch in dataset.batch(batch_size):\n",
    "                model.fit(batch.inputs, batch.targets)\n",
    "        model.save()\n",
    "        model_exists = True\n",
    "        #Build model to sample with\n",
    "        model = RNNTextGenerator(\n",
    "            len(start_seed),\n",
    "        dataset.vocab_size,\n",
    "            name=model_name,\n",
    "        )\n",
    "        model.restore()\n",
    "        #Sample stuff\n",
    "        print('>>>>> {}'.format(start_seed), RNNTextGenerator.sample(\n",
    "            model,\n",
    "            dataset,\n",
    "            start_seed,\n",
    "            50\n",
    "        ))\n",
    "        print('<<<<<<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test(dataset = dataset, learning_rate = learning_rate, start_seed = \".\\n\", model_name = \"boo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
