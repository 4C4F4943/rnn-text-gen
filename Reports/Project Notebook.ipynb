{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "To prepare our data for use by our neural net, we first needed to split it into groups of data that follow specific rules. To streamline the process, we used the `Dataset` class to store and manage our input data. This class was responsible for splitting the data into strings of the correct length and for turning them into one hot encoded arrays that the neural net could better understand. We stored this pre-prepared data in a `Batch` object, which has `inputs` and `targets` attributes for our model to use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, seqs):\n",
    "        \"\"\"Create a batch using the sequence\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "            seqs: int[][]\n",
    "                The one-hot encoded sequences.\n",
    "        \"\"\"\n",
    "        self.inputs = [seq[:-1] for seq in seqs]\n",
    "        self.targets = [seq[1:] for seq in seqs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(\n",
    "            self,\n",
    "            filenames,\n",
    "            seq_length,\n",
    "            shuffle=True,\n",
    "            buffer_size=10000,\n",
    "    ):\n",
    "        \"\"\"Creates a dataset\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "            filenames: string\n",
    "                Path to one or more plain text files.\n",
    "                The file contents are concatenated in the given order.\n",
    "\n",
    "            seq_length: int\n",
    "                The length of the text sequence.\n",
    "\n",
    "            shuffle: boolean\n",
    "                Whether to shuffle the sequences for the batches.\n",
    "\n",
    "            buffer_size: int\n",
    "                 The number of elements from this dataset from which the new\n",
    "                 dataset will sample.\n",
    "        \"\"\"\n",
    "        text = ''\n",
    "        vocab = set()\n",
    "        for filename in filenames:\n",
    "            content = open(filename).read()\n",
    "            text += content\n",
    "            vocab = vocab.union(set(content))\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.char_to_ix = {c: i for i, c in enumerate(vocab)}\n",
    "        self.ix_to_char = list(vocab)\n",
    "        self.text = text\n",
    "        self.data = np.array([self.char_to_ix[c] for c in text])\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def batch(\n",
    "            self,\n",
    "            batch_size,\n",
    "            drop_remainder=True\n",
    "    ):\n",
    "        \"\"\"Batch the instances\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "            batch_size: int\n",
    "                The number of instances in a single batch.\n",
    "\n",
    "            drop_remainder: boolean\n",
    "                Whether the last batch should be dropped in the case its has\n",
    "                fewer than batch_size elements.\n",
    "        \"\"\"\n",
    "        n_seq = len(self.data) // self.seq_length\n",
    "        n_batch = n_seq // batch_size\n",
    "        seq_ids = np.arange(n_seq)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(seq_ids)\n",
    "        i = 0\n",
    "        for _ in range(n_batch):\n",
    "            seqs = [None] * batch_size\n",
    "            for j in range(batch_size):\n",
    "                k = seq_ids[i] * self.seq_length\n",
    "                seqs[j] = self._create_seq(k, k + self.seq_length + 1)\n",
    "                i += 1\n",
    "            yield Batch(seqs)\n",
    "        if not drop_remainder:\n",
    "            seqs = []\n",
    "            for j in range(n_seq % batch_size):\n",
    "                k = seq_ids[i] * self.seq_length\n",
    "                seqs[j] = self._create_seq(k, k + self.seq_length + 1)\n",
    "                i += 1\n",
    "            yield Batch(seqs)\n",
    "\n",
    "    def _create_seq(self, i, j):\n",
    "        return list(map(self._to_label, self.data[i:j]))\n",
    "\n",
    "    def _to_label(self, index):\n",
    "        label = np.zeros(self.vocab_size)\n",
    "        label[index] = 1.0\n",
    "        return label\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"One-hot encode the text\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "            text: string\n",
    "                The text to encode.\n",
    "\n",
    "        Returns\n",
    "        ======================================================================\n",
    "            seq: int[]\n",
    "                The one-hot encoded sequence.\n",
    "        \"\"\"\n",
    "        return [self._to_label(self.char_to_ix[c]) for c in text]\n",
    "\n",
    "    def decode(self, seq):\n",
    "        \"\"\"Decode the one-hot encoded sequence to text format\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "            text: string\n",
    "                The text to encode.\n",
    "\n",
    "        Returns\n",
    "        ======================================================================\n",
    "            seq: int[]\n",
    "                The one-hot encoded sequence.\n",
    "        \"\"\"\n",
    "        text = ''\n",
    "        for label in seq:\n",
    "            text += self.ix_to_char[np.argmax(label)]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text generator itself is stored in the `RNNTextGenerator` class. Among other things, storing the generator in the class allows the session to be stored and used again and prevents accidental loss of information. It also allows multiple generators to exist simultaniously for testing or training with different data. \n",
    "\n",
    "The class also internalizes the methods needed to save and restore a model, allowing the generator to pick up where it previously left off.\n",
    "\n",
    "The text generator does not take batches when training, however, and needs to be fed the inputs and targets seperately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTextGenerator:\n",
    "    \"\"\"A text generator using basic cell and dynamic rnn\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            seq_length,\n",
    "            vocab_size,\n",
    "            rnn_cell=tf.nn.rnn_cell.BasicRNNCell,\n",
    "            n_neurons=100,\n",
    "            optimizer=tf.train.AdamOptimizer,\n",
    "            learning_rate=0.001,\n",
    "            name='RNNTextGenerator',\n",
    "            logdir=None\n",
    "    ):\n",
    "        \"\"\"Initialize the text generator and contruct the tf graph\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        seq_length: int\n",
    "            The number of characters in a sequence.\n",
    "\n",
    "        vocab_size: int\n",
    "            The number of unique characters in the text.\n",
    "\n",
    "        neurons_per_cell: int\n",
    "            The number of neurons in each RNN cell.\n",
    "\n",
    "        name: string\n",
    "            The name of the net (for graph visualization in tensorboard).\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.tf_graph = tf.Graph()\n",
    "        with self.tf_graph.as_default():\n",
    "            self.tf_sess = tf.Session()\n",
    "            # One-hot encoded input and targets\n",
    "            \"\"\"placeholder\n",
    "            Example\n",
    "            [\n",
    "                batch_0: [\n",
    "                    seq_0: [\n",
    "                        # encoded labels with 5 categories\n",
    "                        [0, 0, 0, 1, 0],  # i = 0\n",
    "                        [0, 0, 1, 0, 0],  # i = 1\n",
    "                    ],\n",
    "                    ...\n",
    "                ],\n",
    "                ...\n",
    "            ]\n",
    "            \"\"\"\n",
    "            self.tf_input = tf.placeholder(\n",
    "                tf.float32, shape=(None, seq_length, vocab_size)\n",
    "            )\n",
    "            self.tf_target = tf.placeholder(\n",
    "                tf.float32, shape=(None, seq_length, vocab_size)\n",
    "            )\n",
    "            with tf.variable_scope(name):\n",
    "                self.tf_rnn_cell = rnn_cell(n_neurons)\n",
    "                outputs, _ = tf.nn.dynamic_rnn(\n",
    "                    self.tf_rnn_cell,\n",
    "                    tf.cast(self.tf_input, tf.float32),\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "                logits = tf.layers.dense(outputs, vocab_size)\n",
    "                self.tf_loss = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=logits,\n",
    "                        labels=self.tf_target,\n",
    "                    )\n",
    "                )\n",
    "                self.tf_train = optimizer(\n",
    "                    learning_rate=learning_rate\n",
    "                ).minimize(self.tf_loss)\n",
    "                # Normilize the probablities\n",
    "                y = tf.math.exp(logits)\n",
    "                self.tf_prob = y / tf.reduce_sum(y, 2, keep_dims=True)\n",
    "                self.tf_acc = tf.reduce_mean(tf.cast(\n",
    "                    tf.equal(\n",
    "                        tf.argmax(logits, 2),\n",
    "                        tf.argmax(self.tf_target, 2),\n",
    "                    ),\n",
    "                    tf.float32\n",
    "                ))\n",
    "                self.tf_saver = tf.train.Saver()\n",
    "                if logdir is not None:\n",
    "                    self.logger = tf.summary.FileWriter(logdir, self.tf_graph)\n",
    "            # Initialize the tf session\n",
    "            self.tf_sess.run(tf.global_variables_initializer())\n",
    "            self.tf_sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    def fit(self, inputs, targets):\n",
    "        \"\"\"Fit and train the classifier with a batch of inputs and targets\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        inputs: np.ndarray\n",
    "            A batch of input sequences.\n",
    "\n",
    "        targets: np.ndarray\n",
    "            A batch of target sequences.\n",
    "        \"\"\"\n",
    "        self.tf_sess.run(\n",
    "            self.tf_train,\n",
    "            feed_dict={\n",
    "                self.tf_input: inputs,\n",
    "                self.tf_target: targets,\n",
    "            },\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def score(self, inputs, targets):\n",
    "        \"\"\"Get the score for the batch\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        inputs: np.ndarray\n",
    "            A batch of input sequences.\n",
    "\n",
    "        targets: np.ndarray\n",
    "            A batch of target sequences.\n",
    "\n",
    "        Returns\n",
    "        ======================================================================\n",
    "        accuracy: tf.float32\n",
    "            The accuracy on this batch.\n",
    "\n",
    "        loss: tf.float32\n",
    "            The loss on this batch.\n",
    "        \"\"\"\n",
    "        return self.tf_sess.run(\n",
    "            [self.tf_acc, self.tf_loss],\n",
    "            feed_dict={\n",
    "                self.tf_input: inputs,\n",
    "                self.tf_target: targets,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"Predict the probablities for the labels, for a batch of inputs\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        inputs: np.ndarray\n",
    "            A batch of input sequences.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ======================================================================\n",
    "        predictions: np.ndarray\n",
    "            A batch of sequences of probablities.\n",
    "        \"\"\"\n",
    "        return self.tf_sess.run(\n",
    "            self.tf_prob,\n",
    "            feed_dict={\n",
    "                self.tf_input: inputs,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def save(self, path='./model'):\n",
    "        \"\"\"Save the model\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        path: string\n",
    "            The path to store the model.\n",
    "        \"\"\"\n",
    "        self.tf_saver.save(\n",
    "            self.tf_sess,\n",
    "            path + '/' + self.name\n",
    "        )\n",
    "\n",
    "    def restore(self, path='./model'):\n",
    "        \"\"\"Restore the model\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        path: string\n",
    "            The path to store the weights.\n",
    "        \"\"\"\n",
    "        self.tf_saver.restore(\n",
    "            self.tf_sess,\n",
    "            path + '/' + self.name\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(model, dataset, start_seq, length):\n",
    "        \"\"\"Generate the text using a saved model\n",
    "        Arguments\n",
    "        ======================================================================\n",
    "        model: RNNTextGenerator\n",
    "            The model to sample from.\n",
    "\n",
    "        dataset: Dataset\n",
    "            The dataset to encode and decode the labels.\n",
    "\n",
    "        start_seq: int[]\n",
    "            The sequence to begin with.\n",
    "\n",
    "        length: int\n",
    "            The length of the generated text.\n",
    "\n",
    "        Returns\n",
    "        ======================================================================\n",
    "        text: int[]\n",
    "            The one-hot encoded character labels.\n",
    "        \"\"\"\n",
    "        text = [None] * length\n",
    "        seq = dataset.encode(start_seq)\n",
    "        for i in range(length):\n",
    "            ix = np.random.choice(\n",
    "                range(dataset.vocab_size),\n",
    "                # pred[batch 0][last item in the sequence]\n",
    "                p=model.predict([seq])[0][-1]\n",
    "            )\n",
    "            x = np.zeros(dataset.vocab_size)\n",
    "            x[ix] = 1\n",
    "            del seq[0]\n",
    "            seq.append(x)\n",
    "            text[i] = x\n",
    "        return dataset.decode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Testing Dataset generator-----------\n"
     ]
    }
   ],
   "source": [
    "def test_batch_a_seq():\n",
    "        print(\"-----------Testing Dataset generator-----------\")\n",
    "        batch_size = 5\n",
    "        seq_length = 100\n",
    "        filename = '../data/alice.txt'\n",
    "        dataset = Dataset([filename], seq_length)\n",
    "        for batch in dataset.batch(batch_size):\n",
    "            assert(len(batch.inputs) == batch_size)\n",
    "            assert(len(batch.targets) == batch_size)\n",
    "            assert(len(batch.inputs[-1]) == seq_length)\n",
    "            assert(len(batch.targets[-1]) == seq_length)\n",
    "test_batch_a_seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the text generator\n",
    "Before using any neural net, it's importamt to make sure that it is correctly processing the data it is given. To make sure this is the case, we fed our text generator some randomly generated data. While a fresh model will be needed to train on the text, this model is used to verify that the code is working correctly.\n",
    "In this case, we are checking to ensure the model is providing sufficient variance in its outputs. This shows that the model is opperating on its inputs and successfully completing its operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_label(vocab_size):\n",
    "    \"\"\"randomly assign a label\n",
    "    \"\"\"\n",
    "    label = np.random.randint(vocab_size)\n",
    "    seq = np.zeros(vocab_size)\n",
    "    seq[label] = 1.0\n",
    "    return seq\n",
    "\n",
    "\n",
    "def random_data(batch_size, seq_length, vocab_size):\n",
    "    \"\"\"generate random data\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for _ in range(batch_size):\n",
    "        labels = [random_label(vocab_size) for _ in range(seq_length + 1)]\n",
    "        inputs.append(labels[:-1])\n",
    "        targets.append(labels[1:])\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "def test_on_random_data():\n",
    "    print(\"---------------Testing text generator with randomly generated data-------------\")\n",
    "    seq_length = 10\n",
    "    vocab_size = 4\n",
    "    batch_size = 2\n",
    "    text_gen = RNNTextGenerator(\n",
    "        seq_length,\n",
    "        vocab_size,\n",
    "    )\n",
    "    print('first fit')\n",
    "    inputs, targets = random_data(batch_size, seq_length, vocab_size)\n",
    "    print('fit:', text_gen.fit(inputs, targets))\n",
    "    print('score:', text_gen.score(inputs, targets))\n",
    "    print('predictions:', text_gen.predict(inputs))\n",
    "    print('true targets:', targets)\n",
    "    text_gen.save()\n",
    "\n",
    "    seq_length = 5\n",
    "    text_gen = RNNTextGenerator(\n",
    "        seq_length,\n",
    "        vocab_size\n",
    "    )\n",
    "    text_gen.restore()\n",
    "\n",
    "def test_log():\n",
    "    print(\"-------------Testing logs---------------\")\n",
    "    seq_length = 10\n",
    "    vocab_size = 4\n",
    "    batch_size = 2\n",
    "    text_gen = RNNTextGenerator(\n",
    "        seq_length,\n",
    "        vocab_size,\n",
    "        logdir='./tf_logs'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Testing text generator with randomly generated data-------------\n",
      "WARNING:tensorflow:From <ipython-input-4-396a028e528c>:56: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-396a028e528c>:66: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-396a028e528c>:74: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "first fit\n",
      "fit: <__main__.RNNTextGenerator object at 0x7fe5622454a8>\n",
      "score: [0.3, 1.3757889]\n",
      "predictions: [[[0.25362095 0.20889768 0.25565118 0.28183022]\n",
      "  [0.21944472 0.19302015 0.3110655  0.27646965]\n",
      "  [0.22584444 0.2517532  0.26989004 0.2525123 ]\n",
      "  [0.28246793 0.20647362 0.3006791  0.21037933]\n",
      "  [0.2666454  0.27356318 0.21455306 0.2452383 ]\n",
      "  [0.24525431 0.15664294 0.31523573 0.28286698]\n",
      "  [0.23202464 0.18302342 0.27027434 0.3146777 ]\n",
      "  [0.32244042 0.20671801 0.24668536 0.22415623]\n",
      "  [0.22302635 0.27802995 0.29371044 0.20523322]\n",
      "  [0.22718678 0.26269305 0.23594728 0.27417287]]\n",
      "\n",
      " [[0.25456113 0.25292167 0.2400319  0.2524853 ]\n",
      "  [0.26160744 0.2002047  0.30484965 0.2333382 ]\n",
      "  [0.33019176 0.22721566 0.21504377 0.22754884]\n",
      "  [0.30163014 0.26261112 0.22193141 0.21382743]\n",
      "  [0.2682858  0.19941568 0.2764041  0.25589442]\n",
      "  [0.2865587  0.30451387 0.22286431 0.18606317]\n",
      "  [0.26818314 0.23898584 0.25534502 0.23748595]\n",
      "  [0.31090295 0.22666803 0.2329087  0.2295203 ]\n",
      "  [0.3247498  0.21297145 0.19022727 0.27205145]\n",
      "  [0.27856967 0.24972656 0.2424351  0.22926868]]]\n",
      "true targets: [[[0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]]]\n",
      "INFO:tensorflow:Restoring parameters from ./model/RNNTextGenerator\n",
      "-------------Testing logs---------------\n"
     ]
    }
   ],
   "source": [
    "test_on_random_data()\n",
    "test_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the model'spredictions as to what the target are have sufficient variance so as to provide seemingly random probabilities of each output. This suggests that the model, while in need of training, is making its predictions appropriatly.\n",
    "We also verified that tensorflow's logs were being stored in the correct location, as that is also managed by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"An end to end test using \n",
    " ALICE'S ADVENTURES IN WONDERLAND\n",
    "\"\"\"\n",
    "\n",
    "class TestAlice(unittest.TestCase):\n",
    "    def test_alice(self):\n",
    "        print(\"----------------Testing dataset Alice---------------\")\n",
    "        seq_length = 25\n",
    "        batch_size = 25\n",
    "        learning_rate = 0.01\n",
    "        epoch = 10\n",
    "        dataset = Dataset(['./data/alice.txt'], seq_length)\n",
    "        model = RNNTextGenerator(\n",
    "            seq_length,\n",
    "            dataset.vocab_size,\n",
    "            learning_rate=learning_rate\n",
    "        )\n",
    "        for _ in range(epoch):\n",
    "            for batch in dataset.batch(batch_size):\n",
    "                model.fit(batch.inputs, batch.targets)\n",
    "        model.save()\n",
    "        start_seq = 'hello'\n",
    "        model = RNNTextGenerator(\n",
    "            len(start_seq),\n",
    "            dataset.vocab_size,\n",
    "        )\n",
    "        model.restore()\n",
    "        print('>>>>> {}'.format(start_seq), RNNTextGenerator.sample(\n",
    "            model,\n",
    "            dataset,\n",
    "            start_seq,\n",
    "            50\n",
    "        ))\n",
    "        print('<<<<<<')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
